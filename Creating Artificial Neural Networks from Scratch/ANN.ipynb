{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating neural network from scratch with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference link for the code snippet- https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference link for the dataset - https://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest_pain</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>serum_cholestoral</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>resting_ecg_results</th>\n",
       "      <th>max_heart_rate_achieved</th>\n",
       "      <th>exercise_induced_angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope of the peak</th>\n",
       "      <th>num_of_major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  chest_pain  resting_blood_pressure  serum_cholestoral  \\\n",
       "0   70.0  1.0         4.0                   130.0              322.0   \n",
       "1   67.0  0.0         3.0                   115.0              564.0   \n",
       "2   57.0  1.0         2.0                   124.0              261.0   \n",
       "3   64.0  1.0         4.0                   128.0              263.0   \n",
       "4   74.0  0.0         2.0                   120.0              269.0   \n",
       "5   65.0  1.0         4.0                   120.0              177.0   \n",
       "6   56.0  1.0         3.0                   130.0              256.0   \n",
       "7   59.0  1.0         4.0                   110.0              239.0   \n",
       "8   60.0  1.0         4.0                   140.0              293.0   \n",
       "9   63.0  0.0         4.0                   150.0              407.0   \n",
       "10  59.0  1.0         4.0                   135.0              234.0   \n",
       "11  53.0  1.0         4.0                   142.0              226.0   \n",
       "12  44.0  1.0         3.0                   140.0              235.0   \n",
       "13  61.0  1.0         1.0                   134.0              234.0   \n",
       "14  57.0  0.0         4.0                   128.0              303.0   \n",
       "15  71.0  0.0         4.0                   112.0              149.0   \n",
       "16  46.0  1.0         4.0                   140.0              311.0   \n",
       "17  53.0  1.0         4.0                   140.0              203.0   \n",
       "18  64.0  1.0         1.0                   110.0              211.0   \n",
       "19  40.0  1.0         1.0                   140.0              199.0   \n",
       "20  67.0  1.0         4.0                   120.0              229.0   \n",
       "21  48.0  1.0         2.0                   130.0              245.0   \n",
       "22  43.0  1.0         4.0                   115.0              303.0   \n",
       "23  47.0  1.0         4.0                   112.0              204.0   \n",
       "24  54.0  0.0         2.0                   132.0              288.0   \n",
       "25  48.0  0.0         3.0                   130.0              275.0   \n",
       "26  46.0  0.0         4.0                   138.0              243.0   \n",
       "27  51.0  0.0         3.0                   120.0              295.0   \n",
       "28  58.0  1.0         3.0                   112.0              230.0   \n",
       "29  71.0  0.0         3.0                   110.0              265.0   \n",
       "\n",
       "    fasting_blood_sugar  resting_ecg_results  max_heart_rate_achieved  \\\n",
       "0                   0.0                  2.0                    109.0   \n",
       "1                   0.0                  2.0                    160.0   \n",
       "2                   0.0                  0.0                    141.0   \n",
       "3                   0.0                  0.0                    105.0   \n",
       "4                   0.0                  2.0                    121.0   \n",
       "5                   0.0                  0.0                    140.0   \n",
       "6                   1.0                  2.0                    142.0   \n",
       "7                   0.0                  2.0                    142.0   \n",
       "8                   0.0                  2.0                    170.0   \n",
       "9                   0.0                  2.0                    154.0   \n",
       "10                  0.0                  0.0                    161.0   \n",
       "11                  0.0                  2.0                    111.0   \n",
       "12                  0.0                  2.0                    180.0   \n",
       "13                  0.0                  0.0                    145.0   \n",
       "14                  0.0                  2.0                    159.0   \n",
       "15                  0.0                  0.0                    125.0   \n",
       "16                  0.0                  0.0                    120.0   \n",
       "17                  1.0                  2.0                    155.0   \n",
       "18                  0.0                  2.0                    144.0   \n",
       "19                  0.0                  0.0                    178.0   \n",
       "20                  0.0                  2.0                    129.0   \n",
       "21                  0.0                  2.0                    180.0   \n",
       "22                  0.0                  0.0                    181.0   \n",
       "23                  0.0                  0.0                    143.0   \n",
       "24                  1.0                  2.0                    159.0   \n",
       "25                  0.0                  0.0                    139.0   \n",
       "26                  0.0                  2.0                    152.0   \n",
       "27                  0.0                  2.0                    157.0   \n",
       "28                  0.0                  2.0                    165.0   \n",
       "29                  1.0                  2.0                    130.0   \n",
       "\n",
       "    exercise_induced_angina  oldpeak  slope of the peak  num_of_major_vessels  \\\n",
       "0                       0.0      2.4                2.0                   3.0   \n",
       "1                       0.0      1.6                2.0                   0.0   \n",
       "2                       0.0      0.3                1.0                   0.0   \n",
       "3                       1.0      0.2                2.0                   1.0   \n",
       "4                       1.0      0.2                1.0                   1.0   \n",
       "5                       0.0      0.4                1.0                   0.0   \n",
       "6                       1.0      0.6                2.0                   1.0   \n",
       "7                       1.0      1.2                2.0                   1.0   \n",
       "8                       0.0      1.2                2.0                   2.0   \n",
       "9                       0.0      4.0                2.0                   3.0   \n",
       "10                      0.0      0.5                2.0                   0.0   \n",
       "11                      1.0      0.0                1.0                   0.0   \n",
       "12                      0.0      0.0                1.0                   0.0   \n",
       "13                      0.0      2.6                2.0                   2.0   \n",
       "14                      0.0      0.0                1.0                   1.0   \n",
       "15                      0.0      1.6                2.0                   0.0   \n",
       "16                      1.0      1.8                2.0                   2.0   \n",
       "17                      1.0      3.1                3.0                   0.0   \n",
       "18                      1.0      1.8                2.0                   0.0   \n",
       "19                      1.0      1.4                1.0                   0.0   \n",
       "20                      1.0      2.6                2.0                   2.0   \n",
       "21                      0.0      0.2                2.0                   0.0   \n",
       "22                      0.0      1.2                2.0                   0.0   \n",
       "23                      0.0      0.1                1.0                   0.0   \n",
       "24                      1.0      0.0                1.0                   1.0   \n",
       "25                      0.0      0.2                1.0                   0.0   \n",
       "26                      1.0      0.0                2.0                   0.0   \n",
       "27                      0.0      0.6                1.0                   0.0   \n",
       "28                      0.0      2.5                2.0                   1.0   \n",
       "29                      0.0      0.0                1.0                   1.0   \n",
       "\n",
       "    thal  heart_disease  \n",
       "0    3.0              2  \n",
       "1    7.0              1  \n",
       "2    7.0              2  \n",
       "3    7.0              1  \n",
       "4    3.0              1  \n",
       "5    7.0              1  \n",
       "6    6.0              2  \n",
       "7    7.0              2  \n",
       "8    7.0              2  \n",
       "9    7.0              2  \n",
       "10   7.0              1  \n",
       "11   7.0              1  \n",
       "12   3.0              1  \n",
       "13   3.0              2  \n",
       "14   3.0              1  \n",
       "15   3.0              1  \n",
       "16   7.0              2  \n",
       "17   7.0              2  \n",
       "18   3.0              1  \n",
       "19   7.0              1  \n",
       "20   7.0              2  \n",
       "21   3.0              1  \n",
       "22   3.0              1  \n",
       "23   3.0              1  \n",
       "24   3.0              1  \n",
       "25   3.0              1  \n",
       "26   3.0              1  \n",
       "27   3.0              1  \n",
       "28   7.0              2  \n",
       "29   3.0              1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Pandas module\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#Add header names\n",
    "headers =  ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "#Import dataset in the form of a dataframe\n",
    "heart_df = pd.read_csv('heart.dat', sep=' ', names=headers)\n",
    "\n",
    "#Looking at the first five rows of the dataset\n",
    "heart_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting shape of the data\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking for the null values through the dataset because neural net require non null variables\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "#Importing essential packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Splitting data into independant and depedant variables\n",
    "\n",
    "X = heart_df.drop(columns=['heart_disease']) #Independant data variables\n",
    "\n",
    "#replace target class with 0 and 1 \n",
    "#1 means \"have heart disease\" and 0 means \"do not have heart disease\"\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1, 0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2, 1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1) #Dependant or target variable\n",
    "\n",
    "#Split data into train and test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size=0.2, random_state=2)\n",
    "\n",
    "#Standardize the dataset\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNet():\n",
    "    '''\n",
    "    A two layer neural network\n",
    "    '''\n",
    "        \n",
    "    def __init__(self, layers=[13,8,1], learning_rate=0.001, iterations=100):\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "                \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialize the weights from a random normal distribution\n",
    "        '''\n",
    "        np.random.seed(1) # Seed the random number generator\n",
    "        self.params[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) \n",
    "        self.params['b1']  =np.random.randn(self.layers[1],)\n",
    "        self.params['W2'] = np.random.randn(self.layers[1],self.layers[2]) \n",
    "        self.params['b2'] = np.random.randn(self.layers[2],)\n",
    "        \n",
    "    def relu(self,Z):\n",
    "        '''\n",
    "        The ReLu activation function is to performs a threshold\n",
    "        operation to each input element where values less \n",
    "        than zero are set to zero.\n",
    "        '''\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        The sigmoid function takes in real numbers in any range and \n",
    "        squashes it to a real-valued output between 0 and 1.\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-Z))\n",
    "    \n",
    "    def entropy_loss(self,y, yhat):\n",
    "        nsample = len(y)\n",
    "        loss = -1/nsample * (np.sum(np.multiply(np.log(yhat), y) + np.multiply((1 - y), np.log(1 - yhat))))\n",
    "        return loss\n",
    "    \n",
    "    def forward_propagation(self):\n",
    "        '''\n",
    "        Performs the forward propagation\n",
    "        '''\n",
    "        \n",
    "        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        yhat = self.sigmoid(Z2)\n",
    "        loss = self.entropy_loss(self.y,yhat)\n",
    "\n",
    "        # save calculated parameters     \n",
    "        self.params['Z1'] = Z1\n",
    "        self.params['Z2'] = Z2\n",
    "        self.params['A1'] = A1\n",
    "\n",
    "        return yhat,loss\n",
    "    \n",
    "    def back_propagation(self,yhat):\n",
    "        '''\n",
    "        Computes the derivatives and update weights and bias according.\n",
    "        '''\n",
    "        def dRelu(x):\n",
    "            x[x<=0] = 0\n",
    "            x[x>0] = 1\n",
    "            return x\n",
    "        \n",
    "        dl_wrt_yhat = -(np.divide(self.y,yhat) - np.divide((1 - self.y),(1-yhat)))\n",
    "        dl_wrt_sig = yhat * (1-yhat)\n",
    "        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n",
    "\n",
    "        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0)\n",
    "\n",
    "        dl_wrt_z1 = dl_wrt_A1 * dRelu(self.params['Z1'])\n",
    "        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0)\n",
    "        \n",
    "        #update the weights and bias\n",
    "        self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n",
    "        self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n",
    "        self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n",
    "        self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the neural network using the specified data and labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights() #initialize weights and bias\n",
    "\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            yhat, loss = self.forward_propagation()\n",
    "            self.back_propagation(yhat)\n",
    "            self.loss.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts on a test data\n",
    "        '''\n",
    "        Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        pred = self.sigmoid(Z2)\n",
    "        return np.round(pred) \n",
    "    \n",
    "    def acc(self, y, yhat):\n",
    "        '''\n",
    "        Calculates the accuracy between the predicted value and the truth labels\n",
    "        '''\n",
    "        acc = int(sum(y == yhat) / len(y) * 100)\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"logloss\")\n",
    "        plt.title(\"Loss curve for training\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create object of class Neural net\n",
    "nn = NeuralNet(layers=[13,8,1], learning_rate=0.001, iterations=300) # create the NN model\n",
    "\n",
    "#Fit the train dataset to this object - training the model\n",
    "nn.fit(Xtrain, ytrain) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss \n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 92\n",
      "Test accuracy is 74\n"
     ]
    }
   ],
   "source": [
    "#Predict the train and test data \n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)\n",
    "\n",
    "#Calculate the accuracy of the train and test datasets\n",
    "print(\"Train accuracy is {}\".format(nn.acc(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}\".format(nn.acc(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18355248732093069"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.loss[len(nn.loss)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neural net with Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the help of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5d13de84bb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#With Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#With Keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(8,input_shape=(13,)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.9651 - accuracy: 0.4630\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8830 - accuracy: 0.4861\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.8113 - accuracy: 0.5370\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7496 - accuracy: 0.5694\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 663us/step - loss: 0.6465 - accuracy: 0.6435\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.6713\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7037\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7176\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7361\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7546\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7639\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7824\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8009\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8056\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8102\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8148\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8148\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.8241\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8287\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8287\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8333\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8380\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8380\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8426\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8426\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8426\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8472\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8472\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8472\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8426\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8426\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8426\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8472\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8565\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3194 - accuracy: 0.8565\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8565\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8565\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8611\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8611\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8611\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3111 - accuracy: 0.8611\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8657\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3090 - accuracy: 0.8657\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8657\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8657\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3061 - accuracy: 0.8611\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8611\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8657\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8657\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8657\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.8657\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8611\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8657\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8657\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8657\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.8657\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.8657\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 891us/step - loss: 0.2984 - accuracy: 0.8657\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8657\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8657\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8657\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8657\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8704\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8704\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8704\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8704\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8704\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 992us/step - loss: 0.2940 - accuracy: 0.8704\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.8704\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.8704\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8704\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8704\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8704\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8704\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8704\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8704\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8750\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8750\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8750\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8750\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8750\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 987us/step - loss: 0.2900 - accuracy: 0.8750\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8750\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8750\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8750\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8750\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2887 - accuracy: 0.8750\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2883 - accuracy: 0.8750\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8750\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8750\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8750\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 994us/step - loss: 0.2875 - accuracy: 0.8750\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8750\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8750\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8750\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.8750\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8750\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2866 - accuracy: 0.8750\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8750\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5699 - accuracy: 0.7778\n",
      "Train accuracy of keras neural network: 87.5\n",
      "Test accuracy of keras neural network: 77.78\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model to data - Training \n",
    "model.fit(Xtrain, ytrain, epochs=100, verbose=1)\n",
    "\n",
    "#Calculate the train accuracy\n",
    "train_acc = model.evaluate(Xtrain, ytrain, verbose=1)[1]\n",
    "# print(train_acc)\n",
    "\n",
    "#Calculate the test accuracy\n",
    "test_acc = model.evaluate(Xtest, ytest, verbose=1)[1]\n",
    "# print(test_acc)\n",
    "\n",
    "print(\"Train accuracy of keras neural network: {}\".format(round((train_acc * 100), 2)))\n",
    "print(\"Test accuracy of keras neural network: {}\".format(round((test_acc * 100),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#How to save Keras models\n",
    "#save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 112       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#How to load saved keras model and use it again\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    " \n",
    "#Load model\n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "#Summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference link for how to save models and load them again - https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(Xtest),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
